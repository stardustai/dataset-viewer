use tauri_plugin_http;
use base64::{Engine as _, engine::general_purpose};
use tauri_plugin_dialog::DialogExt;
use tokio::io::AsyncWriteExt;
use futures_util::StreamExt;
use tauri::Emitter;
use reqwest;
use std::sync::{Arc, Mutex};
use std::collections::HashMap;
use tokio::sync::broadcast;
use std::sync::LazyLock;

mod compression;
use compression::{ArchiveAnalyzer, ArchiveInfo, FilePreview};

// å…¨å±€ä¸‹è½½ç®¡ç†å™¨
static DOWNLOAD_MANAGER: LazyLock<Arc<Mutex<HashMap<String, broadcast::Sender<()>>>>> =
    LazyLock::new(|| Arc::new(Mutex::new(HashMap::new())));

// Learn more about Tauri commands at https://tauri.app/develop/calling-rust/
#[tauri::command]
fn greet(name: &str) -> String {
    format!("Hello, {}! You've been greeted from Rust!", name)
}

#[tauri::command]
async fn webdav_request(
    method: String,
    url: String,
    headers: std::collections::HashMap<String, String>,
    body: Option<String>,
) -> Result<serde_json::Value, String> {
    let client = tauri_plugin_http::reqwest::Client::new();

    let mut request = match method.as_str() {
        "GET" => client.get(&url),
        "POST" => client.post(&url),
        "PUT" => client.put(&url),
        "DELETE" => client.delete(&url),
        "HEAD" => client.head(&url),
        "PROPFIND" => client.request(tauri_plugin_http::reqwest::Method::from_bytes(b"PROPFIND").unwrap(), &url),
        _ => return Err(format!("Unsupported method: {}", method)),
    };

    // æ·»åŠ headers
    for (key, value) in headers {
        request = request.header(&key, &value);
    }

    // æ·»åŠ body
    if let Some(body_content) = body {
        request = request.body(body_content);
    }

    // å‘é€è¯·æ±‚
    let response = request.send().await.map_err(|e| format!("Request failed: {}", e))?;

    let status = response.status().as_u16();
    let headers_map: std::collections::HashMap<String, String> = response
        .headers()
        .iter()
        .map(|(k, v)| (k.to_string(), v.to_str().unwrap_or("").to_string()))
        .collect();

    let text = response.text().await.map_err(|e| format!("Failed to read response: {}", e))?;

    Ok(serde_json::json!({
        "status": status,
        "headers": headers_map,
        "body": text
    }))
}

#[tauri::command]
async fn webdav_request_binary(
    method: String,
    url: String,
    headers: std::collections::HashMap<String, String>,
    body: Option<String>,
) -> Result<serde_json::Value, String> {
    let client = tauri_plugin_http::reqwest::Client::new();

    let mut request = match method.as_str() {
        "GET" => client.get(&url),
        "POST" => client.post(&url),
        "PUT" => client.put(&url),
        "DELETE" => client.delete(&url),
        "HEAD" => client.head(&url),
        "PROPFIND" => client.request(tauri_plugin_http::reqwest::Method::from_bytes(b"PROPFIND").unwrap(), &url),
        _ => return Err(format!("Unsupported method: {}", method)),
    };

    // æ·»åŠ headers
    for (key, value) in headers {
        request = request.header(&key, &value);
    }

    // æ·»åŠ body
    if let Some(body_content) = body {
        request = request.body(body_content);
    }

    // å‘é€è¯·æ±‚
    let response = request.send().await.map_err(|e| format!("Request failed: {}", e))?;

    let status = response.status().as_u16();
    let headers_map: std::collections::HashMap<String, String> = response
        .headers()
        .iter()
        .map(|(k, v)| (k.to_string(), v.to_str().unwrap_or("").to_string()))
        .collect();

    // è·å–äºŒè¿›åˆ¶æ•°æ®å¹¶è½¬æ¢ä¸ºbase64
    let bytes = response.bytes().await.map_err(|e| format!("Failed to read response: {}", e))?;
    let body_base64 = general_purpose::STANDARD.encode(&bytes);

    Ok(serde_json::json!({
        "status": status,
        "headers": headers_map,
        "body": body_base64
    }))
}

#[tauri::command]
async fn download_file_with_progress(
    app: tauri::AppHandle,
    method: String,
    url: String,
    headers: std::collections::HashMap<String, String>,
    filename: String,
) -> Result<String, String> {
    // æ˜¾ç¤ºä¿å­˜æ–‡ä»¶å¯¹è¯æ¡†
    let file_path = app.dialog()
        .file()
        .set_file_name(&filename)
        .blocking_save_file();

    let save_path = match file_path {
        Some(path) => path.into_path().map_err(|e| format!("Failed to get path: {}", e))?,
        None => return Err("User cancelled file save".to_string()),
    };

    let client = reqwest::Client::new();

    let mut request_builder = match method.as_str() {
        "GET" => client.get(&url),
        "POST" => client.post(&url),
        "PUT" => client.put(&url),
        "DELETE" => client.delete(&url),
        "HEAD" => client.head(&url),
        "PROPFIND" => client.request(reqwest::Method::from_bytes(b"PROPFIND").unwrap(), &url),
        _ => return Err(format!("Unsupported method: {}", method)),
    };

    // æ·»åŠ headers
    for (key, value) in headers {
        request_builder = request_builder.header(&key, &value);
    }

    // å‘é€è¯·æ±‚å¹¶è·å–å“åº”
    let response = request_builder.send().await.map_err(|e| {
        format!("Request failed: {}", e)
    })?;

    if !response.status().is_success() {
        return Err(format!("Download failed with status: {} - {}",
            response.status().as_u16(),
            response.status().canonical_reason().unwrap_or("Unknown error")));
    }

    // è·å–æ–‡ä»¶æ€»å¤§å°
    let total_size = response.content_length().unwrap_or(0);

    // åˆ›å»ºå–æ¶ˆä¿¡å·
    let (cancel_tx, mut cancel_rx) = broadcast::channel::<()>(1);

    // å°†å–æ¶ˆå‘é€å™¨å­˜å‚¨åˆ°å…¨å±€ç®¡ç†å™¨
    {
        let mut manager = DOWNLOAD_MANAGER.lock().unwrap();
        manager.insert(filename.clone(), cancel_tx);
    }

    // å‘é€å¼€å§‹ä¸‹è½½äº‹ä»¶
    let _ = app.emit("download-started", serde_json::json!({
        "filename": filename,
        "total_size": total_size
    }));

    // åˆ›å»ºæ–‡ä»¶
    let mut file = tokio::fs::File::create(&save_path)
        .await
        .map_err(|e| format!("Failed to create file: {}", e))?;

    // çœŸæ­£çš„æµå¼ä¸‹è½½ - é€å—è¯»å–å’Œå†™å…¥
    let mut downloaded: u64 = 0;
    let mut stream = response.bytes_stream();

    while let Some(chunk_result) = stream.next().await {
        // æ£€æŸ¥æ˜¯å¦æ”¶åˆ°å–æ¶ˆä¿¡å·
        if cancel_rx.try_recv().is_ok() {
            // åˆ é™¤éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶
            let _ = tokio::fs::remove_file(&save_path).await;

            // ä»ç®¡ç†å™¨ä¸­ç§»é™¤
            {
                let mut manager = DOWNLOAD_MANAGER.lock().unwrap();
                manager.remove(&filename);
            }

            // å‘é€å–æ¶ˆäº‹ä»¶
            let _ = app.emit("download-error", serde_json::json!({
                "filename": filename,
                "error": "Download cancelled by user"
            }));

            return Err("Download cancelled by user".to_string());
        }

        let chunk = chunk_result.map_err(|e| format!("Failed to read chunk: {}", e))?;

        // å†™å…¥æ–‡ä»¶
        file.write_all(&chunk)
            .await
            .map_err(|e| format!("Failed to write chunk: {}", e))?;

        downloaded += chunk.len() as u64;

        // å‘é€è¿›åº¦æ›´æ–°äº‹ä»¶ï¼ˆæ¯64KBæˆ–æ¯å—æ›´æ–°ä¸€æ¬¡ï¼‰
        let progress = if total_size > 0 {
            (downloaded as f64 / total_size as f64 * 100.0).round() as u32
        } else {
            0
        };

        // åªåœ¨è¿›åº¦æœ‰æ˜¾è‘—å˜åŒ–æ—¶å‘é€äº‹ä»¶ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„æ›´æ–°
        if downloaded % (64 * 1024) == 0 || chunk.len() < 64 * 1024 {
            let _ = app.emit("download-progress", serde_json::json!({
                "filename": filename,
                "downloaded": downloaded,
                "total_size": total_size,
                "progress": progress
            }));
        }
    }

    // ä¸‹è½½å®Œæˆï¼Œä»ç®¡ç†å™¨ä¸­ç§»é™¤
    {
        let mut manager = DOWNLOAD_MANAGER.lock().unwrap();
        manager.remove(&filename);
    }

    file.flush().await.map_err(|e| format!("Failed to flush file: {}", e))?;

    // å‘é€å®Œæˆäº‹ä»¶
    let _ = app.emit("download-completed", serde_json::json!({
        "filename": filename,
        "file_path": save_path.display().to_string()
    }));

    Ok(format!("File downloaded successfully to: {}", save_path.display()))
}

#[tauri::command]
async fn cancel_download(filename: String) -> Result<String, String> {
    let mut manager = DOWNLOAD_MANAGER.lock().unwrap();

    if let Some(cancel_sender) = manager.remove(&filename) {
        // å‘é€å–æ¶ˆä¿¡å·
        let _ = cancel_sender.send(());
        Ok(format!("Download cancellation signal sent for: {}", filename))
    } else {
        Err(format!("No active download found for: {}", filename))
    }
}

/// åˆ†æå‹ç¼©æ–‡ä»¶ç»“æ„
#[tauri::command]
async fn analyze_compressed_file(
    url: String,
    headers: std::collections::HashMap<String, String>,
    filename: String,
    max_size: Option<u64>,
) -> Result<ArchiveInfo, String> {
    let client = tauri_plugin_http::reqwest::Client::new();
    let lower_filename = filename.to_lowercase();

    // å¯¹äº ZIP æ–‡ä»¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
    if lower_filename.ends_with(".zip") {
        return analyze_zip_file_optimized(client, url, headers, filename, max_size).await;
    }

    let mut request = client.get(&url);

    // æ·»åŠ headers
    for (key, value) in headers {
        request = request.header(&key, &value);
    }

    // å¯¹äºéZIPæ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨ Range è¯·æ±‚åªä¸‹è½½æ–‡ä»¶å¤´éƒ¨
    if let Some(max) = max_size {
        request = request.header("Range", format!("bytes=0-{}", max - 1));
    }

    let response = request.send().await
        .map_err(|e| format!("Failed to fetch compressed file: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("HTTP error: {}", response.status()));
    }

    let data = response.bytes().await
        .map_err(|e| format!("Failed to read response data: {}", e))?;

    ArchiveAnalyzer::analyze_archive(&data, &filename)
}

/// ä¼˜åŒ–çš„ZIPæ–‡ä»¶åˆ†æï¼Œæ”¯æŒå¤§æ–‡ä»¶çš„æ™ºèƒ½å¤„ç†
async fn analyze_zip_file_optimized(
    client: tauri_plugin_http::reqwest::Client,
    url: String,
    headers: std::collections::HashMap<String, String>,
    filename: String,
    _max_size: Option<u64>,
) -> Result<ArchiveInfo, String> {
    // é¦–å…ˆè·å–æ–‡ä»¶å¤§å°
    let mut head_request = client.head(&url);
    for (key, value) in &headers {
        head_request = head_request.header(key, value);
    }

    let head_response = head_request.send().await
        .map_err(|e| format!("Failed to get file info: {}", e))?;

    let content_length = head_response.headers()
        .get("content-length")
        .and_then(|v| v.to_str().ok())
        .and_then(|s| s.parse::<u64>().ok());

    let file_size = content_length.ok_or("Cannot get ZIP file size")?;

    // ä½¿ç”¨æ™ºèƒ½çš„ZIPä¸­å¤®ç›®å½•è¯»å–ç­–ç•¥
    parse_zip_central_directory(client, url, headers, filename, file_size).await
}

/// æ™ºèƒ½è§£æZIPä¸­å¤®ç›®å½•ï¼Œåªè¯»å–å¿…è¦çš„æ–‡ä»¶å°¾éƒ¨æ•°æ®
async fn parse_zip_central_directory(
    client: tauri_plugin_http::reqwest::Client,
    url: String,
    headers: std::collections::HashMap<String, String>,
    filename: String,
    file_size: u64,
) -> Result<ArchiveInfo, String> {
    // ZIP æ–‡ä»¶ç»“æ„ï¼š
    // [Local file headers + file data] ... [Central directory] [End of central directory record]
    //
    // End of central directory record (EOCD) åœ¨æ–‡ä»¶æœ«å°¾ï¼Œæœ€å°22å­—èŠ‚ï¼Œæœ€å¤§65557å­—èŠ‚ï¼ˆåŒ…å«æ³¨é‡Šï¼‰
    // æˆ‘ä»¬å…ˆè¯»å–è¾ƒå°çš„å°¾éƒ¨ï¼Œç„¶åæ ¹æ®éœ€è¦å¢åŠ è¯»å–å¤§å°

    let read_attempts = vec![
        1024u64,      // 1KB - è¶³å¤Ÿè¯»å–ç®€å•çš„ EOCD
        8192,         // 8KB - ä¸­ç­‰å¤§å°çš„æ³¨é‡Š
        65557,        // æœ€å¤§å¯èƒ½çš„ EOCD å¤§å°
        std::cmp::min(1024 * 1024, file_size), // 1MB æˆ–æ–‡ä»¶å¤§å°
    ];

    for &read_size in &read_attempts {
        if read_size > file_size {
            continue;
        }

        let start_pos = file_size - read_size;

        match try_parse_zip_eocd(
            client.clone(),
            url.clone(),
            headers.clone(),
            start_pos,
            file_size,
            &filename
        ).await {
            Ok(archive_info) => return Ok(archive_info),
            Err(e) => {
                // å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œè¿”å›é”™è¯¯
                if read_size == *read_attempts.last().unwrap() {
                    return Err(format!("Failed to parse ZIP central directory: {}", e));
                }
                // å¦åˆ™ç»§ç»­å°è¯•æ›´å¤§çš„è¯»å–èŒƒå›´
                continue;
            }
        }
    }

    Err("Failed to parse ZIP file after all attempts".to_string())
}

/// å°è¯•è§£æZIPæ–‡ä»¶çš„EOCD (End of Central Directory) è®°å½•
async fn try_parse_zip_eocd(
    client: tauri_plugin_http::reqwest::Client,
    url: String,
    headers: std::collections::HashMap<String, String>,
    start_pos: u64,
    file_size: u64,
    filename: &str,
) -> Result<ArchiveInfo, String> {
    // è¯»å–æ–‡ä»¶å°¾éƒ¨æ•°æ®
    let mut request = client.get(&url);
    for (key, value) in headers {
        request = request.header(&key, &value);
    }

    request = request.header("Range", format!("bytes={}-{}", start_pos, file_size - 1));

    let response = request.send().await
        .map_err(|e| format!("Failed to fetch ZIP tail: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("HTTP error: {}", response.status()));
    }

    let tail_data = response.bytes().await
        .map_err(|e| format!("Failed to read ZIP tail: {}", e))?;

    // è§£æ EOCD è®°å½•
    parse_eocd_record(&tail_data, file_size, filename)
}

/// è§£æ End of Central Directory Record
fn parse_eocd_record(data: &[u8], file_size: u64, _filename: &str) -> Result<ArchiveInfo, String> {
    // EOCD ç­¾å: 0x06054b50
    let eocd_signature = [0x50, 0x4b, 0x05, 0x06];

    // ä»åå¾€å‰æœç´¢ EOCD ç­¾å
    for i in (0..data.len().saturating_sub(22)).rev() {
        if data.len() < i + 4 {
            continue;
        }

        if &data[i..i+4] == eocd_signature {
            // æ‰¾åˆ°äº† EOCD è®°å½•
            if data.len() < i + 22 {
                continue; // æ•°æ®ä¸å¤Ÿå®Œæ•´çš„ EOCD è®°å½•
            }

            // è§£æ EOCD å­—æ®µ
            let total_entries = u16::from_le_bytes([data[i + 10], data[i + 11]]) as usize;
            let _central_dir_size = u32::from_le_bytes([
                data[i + 12], data[i + 13], data[i + 14], data[i + 15]
            ]) as u64;
            let _central_dir_offset = u32::from_le_bytes([
                data[i + 16], data[i + 17], data[i + 18], data[i + 19]
            ]) as u64;

            // åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ ArchiveInfoï¼Œæš‚æ—¶ä¸åŒ…å«è¯¦ç»†çš„æ–‡ä»¶åˆ—è¡¨
            // å®é™…çš„æ–‡ä»¶åˆ—è¡¨éœ€è¦è¯»å–ä¸­å¤®ç›®å½•
            return Ok(ArchiveInfo {
                entries: create_placeholder_entries(total_entries),
                total_entries,
                compression_type: "zip".to_string(),
                total_uncompressed_size: 0, // éœ€è¦è¯»å–ä¸­å¤®ç›®å½•æ‰èƒ½è®¡ç®—
                total_compressed_size: file_size,
            });
        }
    }

    Err("EOCD record not found in ZIP file tail".to_string())
}

/// åˆ›å»ºå ä½ç¬¦æ¡ç›®ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æŒ‰éœ€åŠ è½½
fn create_placeholder_entries(count: usize) -> Vec<compression::ArchiveEntry> {
    // è¿”å›ä¸€ä¸ªå ä½ç¬¦ï¼Œè¡¨ç¤ºéœ€è¦è¿›ä¸€æ­¥è§£æ
    vec![compression::ArchiveEntry {
        path: format!("ğŸ“ ZIP Archive ({} files) - Click to load details", count),
        size: 0,
        is_dir: true,
        modified_time: None,
        compressed_size: None,
    }]
}

/// æŒ‰éœ€åŠ è½½ZIPæ–‡ä»¶çš„è¯¦ç»†æ–‡ä»¶åˆ—è¡¨
#[tauri::command]
async fn load_zip_file_details(
    url: String,
    headers: std::collections::HashMap<String, String>,
    filename: String,
) -> Result<ArchiveInfo, String> {
    let client = tauri_plugin_http::reqwest::Client::new();

    // è·å–æ–‡ä»¶å¤§å°
    let mut head_request = client.head(&url);
    for (key, value) in &headers {
        head_request = head_request.header(key, value);
    }

    let head_response = head_request.send().await
        .map_err(|e| format!("Failed to get file info: {}", e))?;

    let file_size = head_response.headers()
        .get("content-length")
        .and_then(|v| v.to_str().ok())
        .and_then(|s| s.parse::<u64>().ok())
        .ok_or("Cannot get ZIP file size")?;

    // è¯»å–ä¸­å¤®ç›®å½•æ¥è·å–å®Œæ•´çš„æ–‡ä»¶åˆ—è¡¨
    load_zip_central_directory(client, url, headers, filename, file_size).await
}

/// åŠ è½½ZIPæ–‡ä»¶çš„å®Œæ•´ä¸­å¤®ç›®å½•
async fn load_zip_central_directory(
    client: tauri_plugin_http::reqwest::Client,
    url: String,
    headers: std::collections::HashMap<String, String>,
    filename: String,
    file_size: u64,
) -> Result<ArchiveInfo, String> {
    // é¦–å…ˆè¯»å– EOCD æ¥è·å–ä¸­å¤®ç›®å½•çš„ä½ç½®å’Œå¤§å°
    let eocd_data = read_zip_eocd(&client, &url, &headers, file_size).await?;
    let (central_dir_offset, central_dir_size, total_entries) = parse_eocd_details(&eocd_data)?;

    // è¯»å–ä¸­å¤®ç›®å½•
    let mut request = client.get(&url);
    for (key, value) in headers {
        request = request.header(&key, &value);
    }

    let end_pos = central_dir_offset + central_dir_size;
    request = request.header("Range", format!("bytes={}-{}", central_dir_offset, end_pos - 1));

    let response = request.send().await
        .map_err(|e| format!("Failed to fetch central directory: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("HTTP error reading central directory: {}", response.status()));
    }

    let central_dir_data = response.bytes().await
        .map_err(|e| format!("Failed to read central directory data: {}", e))?;

    // è§£æä¸­å¤®ç›®å½•è®°å½•
    parse_central_directory_records(&central_dir_data, total_entries, file_size, &filename)
}

/// è¯»å–ZIPæ–‡ä»¶çš„EOCDè®°å½•
async fn read_zip_eocd(
    client: &tauri_plugin_http::reqwest::Client,
    url: &str,
    headers: &std::collections::HashMap<String, String>,
    file_size: u64,
) -> Result<Vec<u8>, String> {
    // è¯»å–æœ€å1KBæ¥æŸ¥æ‰¾EOCD
    let read_size = std::cmp::min(1024, file_size);
    let start_pos = file_size - read_size;

    let mut request = client.get(url);
    for (key, value) in headers {
        request = request.header(key, value);
    }

    request = request.header("Range", format!("bytes={}-{}", start_pos, file_size - 1));

    let response = request.send().await
        .map_err(|e| format!("Failed to fetch EOCD: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("HTTP error reading EOCD: {}", response.status()));
    }

    let data = response.bytes().await
        .map_err(|e| format!("Failed to read EOCD data: {}", e))?;

    Ok(data.to_vec())
}

/// è§£æEOCDè®°å½•çš„è¯¦ç»†ä¿¡æ¯
fn parse_eocd_details(data: &[u8]) -> Result<(u64, u64, usize), String> {
    let eocd_signature = [0x50, 0x4b, 0x05, 0x06];

    for i in (0..data.len().saturating_sub(22)).rev() {
        if data.len() < i + 4 {
            continue;
        }

        if &data[i..i+4] == eocd_signature {
            if data.len() < i + 22 {
                continue;
            }

            let total_entries = u16::from_le_bytes([data[i + 10], data[i + 11]]) as usize;
            let central_dir_size = u32::from_le_bytes([
                data[i + 12], data[i + 13], data[i + 14], data[i + 15]
            ]) as u64;
            let central_dir_offset = u32::from_le_bytes([
                data[i + 16], data[i + 17], data[i + 18], data[i + 19]
            ]) as u64;

            return Ok((central_dir_offset, central_dir_size, total_entries));
        }
    }

    Err("EOCD record not found".to_string())
}

/// è§£æä¸­å¤®ç›®å½•è®°å½•ï¼Œæå–æ–‡ä»¶ä¿¡æ¯
fn parse_central_directory_records(
    data: &[u8],
    expected_entries: usize,
    file_size: u64,
    _filename: &str
) -> Result<ArchiveInfo, String> {
    let mut entries = Vec::new();
    let mut offset = 0;
    let mut total_uncompressed_size = 0;

    // ä¸­å¤®ç›®å½•æ–‡ä»¶å¤´ç­¾å: 0x02014b50
    let cd_signature = [0x50, 0x4b, 0x01, 0x02];

    while offset + 46 <= data.len() && entries.len() < expected_entries {
        // æ£€æŸ¥ç­¾å
        if &data[offset..offset+4] != cd_signature {
            break;
        }

        // è§£æä¸­å¤®ç›®å½•æ–‡ä»¶å¤´
        let compressed_size = u32::from_le_bytes([
            data[offset + 20], data[offset + 21], data[offset + 22], data[offset + 23]
        ]) as u64;

        let uncompressed_size = u32::from_le_bytes([
            data[offset + 24], data[offset + 25], data[offset + 26], data[offset + 27]
        ]) as u64;

        let filename_len = u16::from_le_bytes([data[offset + 28], data[offset + 29]]) as usize;
        let extra_len = u16::from_le_bytes([data[offset + 30], data[offset + 31]]) as usize;
        let comment_len = u16::from_le_bytes([data[offset + 32], data[offset + 33]]) as usize;

        let external_attrs = u32::from_le_bytes([
            data[offset + 38], data[offset + 39], data[offset + 40], data[offset + 41]
        ]);

        // è¯»å–æ–‡ä»¶å
        let filename_start = offset + 46;
        let filename_end = filename_start + filename_len;

        if filename_end > data.len() {
            break;
        }

        let entry_name = String::from_utf8_lossy(&data[filename_start..filename_end]).to_string();

        // åˆ¤æ–­æ˜¯å¦ä¸ºç›®å½•ï¼ˆé€šè¿‡å¤–éƒ¨å±æ€§æˆ–è·¯å¾„æœ«å°¾çš„'/'ï¼‰
        let is_dir = (external_attrs & 0x10) != 0 || entry_name.ends_with('/');

        total_uncompressed_size += uncompressed_size;

        entries.push(compression::ArchiveEntry {
            path: entry_name,
            size: uncompressed_size,
            is_dir,
            modified_time: None, // å¯ä»¥ä»DOSæ—¶é—´æˆ³è§£æï¼Œæš‚æ—¶çœç•¥
            compressed_size: Some(compressed_size),
        });

        // ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªè®°å½•
        offset = filename_end + extra_len + comment_len;
    }

    let total_entries = entries.len();

    Ok(ArchiveInfo {
        entries,
        total_entries,
        compression_type: "zip".to_string(),
        total_uncompressed_size,
        total_compressed_size: file_size,
    })
}

/// ä»å‹ç¼©æ–‡ä»¶ä¸­æå–æ–‡ä»¶é¢„è§ˆ
#[tauri::command]
async fn extract_file_preview_from_archive(
    url: String,
    headers: std::collections::HashMap<String, String>,
    filename: String,
    entry_path: String,
    max_preview_size: Option<usize>,
) -> Result<FilePreview, String> {
    let client = tauri_plugin_http::reqwest::Client::new();

    let mut request = client.get(&url);

    // æ·»åŠ headers
    for (key, value) in headers {
        request = request.header(&key, &value);
    }

    let response = request.send().await
        .map_err(|e| format!("Failed to fetch compressed file: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("HTTP error: {}", response.status()));
    }

    let data = response.bytes().await
        .map_err(|e| format!("Failed to read response data: {}", e))?;

    let preview_size = max_preview_size.unwrap_or(64 * 1024); // é»˜è®¤64KBé¢„è§ˆ

    ArchiveAnalyzer::extract_file_preview(&data, &filename, &entry_path, preview_size)
}

/// æµå¼è¯»å–å‹ç¼©æ–‡ä»¶ä¸­çš„æ–‡ä»¶å†…å®¹
#[tauri::command]
async fn stream_compressed_file(
    app: tauri::AppHandle,
    url: String,
    headers: std::collections::HashMap<String, String>,
    filename: String,
    entry_path: String,
    chunk_size: Option<usize>,
) -> Result<String, String> {
    let client = tauri_plugin_http::reqwest::Client::new();

    let mut request = client.get(&url);

    // æ·»åŠ headers
    for (key, value) in headers {
        request = request.header(&key, &value);
    }

    let response = request.send().await
        .map_err(|e| format!("Failed to fetch compressed file: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("HTTP error: {}", response.status()));
    }

    let data = response.bytes().await
        .map_err(|e| format!("Failed to read response data: {}", e))?;

    let chunk_sz = chunk_size.unwrap_or(8192); // é»˜è®¤8KBå—
    let stream_id = format!("{}:{}", filename, entry_path);
    let stream_id_clone = stream_id.clone();

    // å¯åŠ¨å¼‚æ­¥ä»»åŠ¡è¿›è¡Œåˆ†å—è¯»å–
    tokio::spawn(async move {
        let mut offset = 0;
        let mut chunk_index = 0;

        loop {
            match ArchiveAnalyzer::read_compressed_file_chunks(&data, &filename, &entry_path, offset, chunk_sz) {
                Ok((content, is_eof)) => {
                    // å‘é€å½“å‰å—
                    let _ = app.emit("compressed-file-chunk", serde_json::json!({
                        "stream_id": stream_id_clone,
                        "chunk_index": chunk_index,
                        "content": content,
                        "is_complete": is_eof
                    }));

                    chunk_index += 1;

                    if is_eof {
                        // å‘é€å®Œæˆä¿¡å·
                        let _ = app.emit("compressed-file-complete", serde_json::json!({
                            "stream_id": stream_id_clone,
                            "total_chunks": chunk_index
                        }));
                        break;
                    }

                    offset += content.len();

                    // æ·»åŠ å°å»¶è¿Ÿä»¥é¿å…è¿‡äºé¢‘ç¹çš„äº‹ä»¶å‘é€
                    tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
                }
                Err(error) => {
                    let _ = app.emit("compressed-file-error", serde_json::json!({
                        "stream_id": stream_id_clone,
                        "error": error
                    }));
                    break;
                }
            }
        }
    });

    Ok(stream_id)
}

/// è¯»å–å‹ç¼©æ–‡ä»¶çš„æŒ‡å®šå—
#[tauri::command]
async fn read_compressed_file_chunk(
    url: String,
    headers: std::collections::HashMap<String, String>,
    filename: String,
    entry_path: String,
    offset: usize,
    chunk_size: usize,
) -> Result<serde_json::Value, String> {
    let client = tauri_plugin_http::reqwest::Client::new();

    let mut request = client.get(&url);

    // æ·»åŠ headers
    for (key, value) in headers {
        request = request.header(&key, &value);
    }

    let response = request.send().await
        .map_err(|e| format!("Failed to fetch compressed file: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("HTTP error: {}", response.status()));
    }

    let data = response.bytes().await
        .map_err(|e| format!("Failed to read response data: {}", e))?;

    match ArchiveAnalyzer::read_compressed_file_chunks(&data, &filename, &entry_path, offset, chunk_size) {
        Ok((content, is_eof)) => {
            Ok(serde_json::json!({
                "content": content,
                "is_eof": is_eof,
                "offset": offset,
                "bytes_read": content.len()
            }))
        }
        Err(error) => Err(error)
    }
}
#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub fn run() {
    tauri::Builder::default()
        .plugin(tauri_plugin_opener::init())
        .plugin(tauri_plugin_http::init())
        .plugin(tauri_plugin_fs::init())
        .plugin(tauri_plugin_dialog::init())
        .invoke_handler(tauri::generate_handler![
            greet,
            webdav_request,
            webdav_request_binary,
            download_file_with_progress,
            cancel_download,
            analyze_compressed_file,
            load_zip_file_details,
            extract_file_preview_from_archive,
            stream_compressed_file,
            read_compressed_file_chunk
        ])
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}
